---
title: "HW9"
author: "Zachary Brunell"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
editor_options: 
  markdown: 
    wrap: sentence
---
\newpage

# [GitHub Link](https://github.com/zbrunell/SDS315/blob/main/HW9.Rmd)
# Problem 1

## Part A

### Graph For Opening Size
\vspace*{\fill}
```{r, echo=FALSE, warning=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
library(ggplot2)
library(mosaic)
library(tidyverse)
library(moderndive)
library(dplyr)
solder = read_csv('solder.csv')

gg_opening = ggplot(solder) + 
  geom_boxplot(aes(x = Opening, y = skips, fill = Opening)) +
  labs(
    x = "Opening Size (S = Small, M = Medium, L = Large)",
    y = "Number of Skips",
    title = "Distribution of Skips by Opening Size"
  ) +
  theme(
    plot.title = element_text(face = "bold", size = 18),
    axis.title = element_text(size = 13),
    legend.position = "right"
  )

gg_opening
```
The y-axis depicts the number of skips, the x-axis displays the three different opening types (S, M, L). As displayed in the graph, Large Opening Size yields the lowest mean number of skips. The mean number of skips increases as Opening Size decreases.
\vspace*{\fill}
\newpage

### Graph For Soldering Thickness

\vspace*{\fill}
```{r, echo=FALSE, warning=FALSE, fig.align='center'}
gg_solder = ggplot(solder) + 
  geom_boxplot(aes(x = Solder, y = skips, fill = Solder)) +
  labs(
    x = "Thickness of Alloy Used (Thick Or Thin)",
    y = "Number of Skips",
    title = "Distribution of Skips by Thickness of Alloy Used"
  ) +
  theme(
    plot.title = element_text(face = "bold", size = 18),
    axis.title = element_text(size = 13),
    legend.position = "right"
  )
gg_solder


```
The y-axis is again the number of skips, the x-axis is the thickness of solder used (Thick or Thin). The mean number of skips for the Thick Alloy is lower than that of the Thin option.
\vspace*{\fill}
\newpage

## Part B

```{r, include=TRUE, echo=FALSE, message=FALSE}
lm_solder = lm(skips ~ Opening + Solder + Opening:Solder, data = solder)
regression_table = get_regression_table(lm_solder)
regression_table
```

## Part C

\begin{itemize}
\item The intercept of 0.39 represents the baseline number of skips for a circuit board using a Large Opening and Thick Solder. This is our reference group, with all other effects measured relative to it.

\item The main effect of Medium Opening (OpeningM) is estimated at 2.41, meaning that using a Medium rather than Large opening increases the expected number of skips by about 2.41, assuming the solder type remains thick.

\item The main effect of Small Opening (OpeningS) is estimated at 5.13, meaning that switching from a Large to a Small opening results in approximately 5.13 more skips, holding solder type constant.

\item The main effect of Thin Solder (SolderThin) is 2.28, indicating that using a Thin rather than Thick solder increases expected skips by 2.28, assuming the opening size is large.

\item The interaction between OpeningM and SolderThin is -0.74, which suggests that using both a Medium Opening and Thin Solder together results in 0.74 fewer skips than would be expected by simply summing their individual effects

\item The interaction between OpeningS and SolderThin is 9.65, indicating that the combination of Small Opening and Thin Solder leads to 9.65 more skips than what would be predicted by adding the individual effects of OpeningS and SolderThin

\end{itemize}

## Part D

Based on the analysis, I recommend using a Large Opening with Thick Solder. This combination serves as the reference group in our model, so its effect is represented solely by the intercept, which was approximately 0.39 skips—the lowest average skip count among all combinations. While the interaction between OpeningM and SolderThin was negative, it wasn’t substantial enough to offset the larger skip counts associated with their individual effects.


# Problem 2: Grocery Store Prices

## Part A

```{r, echo=FALSE, message=FALSE}
groceries = read_csv('groceries.csv')

store_avg <- groceries %>%
  group_by(Store) %>%
  summarize(avg_price = mean(Price), .groups = 'drop')
groceries <- groceries %>%
  left_join(store_avg, by = "Store")
gg_groceries = ggplot(store_avg) +
  geom_bar(aes(x = reorder(Store, avg_price), y = avg_price, fill = Store), stat = "identity") +
  labs(
    x = "Store",
    y = "Average Price",
    title = "Average Price by Store"
  ) +
  theme_minimal(base_size = 12) +  
  theme(
    axis.text.y = element_text(face = "bold", size = 10), 
    axis.title = element_text(size = 14),
    plot.title = element_text(face = "bold", size = 16),
    legend.position = "none"
  ) +
  coord_flip()

gg_groceries
```
The graph displays the average price of products sold for several stores in Texas. As is clear, Fiesta and Walmart have some of the lowest average prices, while stores like Whole Foods and Wheatsville Food Co-Op have some of the highest.
\newpage

## Part B

```{r, fig.height= 10, fig.width=10, echo=FALSE, include= TRUE}
grocery_by_product = groceries %>%
  group_by(Product) %>%
  summarize(num_stores = n())

gg_products <- ggplot(grocery_by_product) +
  geom_bar(aes(x = num_stores, y = reorder(Product, num_stores)), 
           stat = "identity", fill = 'steelblue') +
  labs(
    x = "Number of Stores Carrying Product",
    y = "Product",
    title = "Product Availability Across Stores",
    caption = "Some products are available in nearly all stores, while others appear in only a few, which affects average price comparisons (Part A)."
  ) +
  theme_minimal(base_size = 10) +  
  theme(
    axis.text.y = element_text(face = 'bold', size = 10), 
    axis.title = element_text(size = 12),
    plot.title = element_text(face = "bold", size = 14),
    plot.caption = element_text(size = 9, margin = margin(t = 10)),
    legend.position = "none"
  )

gg_products
```
This ordered bar graph displays the number of stores in the dataset that carry each product. Notably, some staple items like eggs and milk are available in all 16 stores, whereas other products, such as Cinnamon Toast, are stocked by only 4 stores. This variation in availability may influence average price comparisons and product accessibility across stores.


## Part C
```{r, include=FALSE, echo=FALSE, message=FALSE}
groceries$Type <- relevel(as.factor(groceries$Type), ref = "Grocery")
lm_groceries <- lm(Price ~ Product + Type, data = groceries)
coef(lm_groceries)
regression_table_GROCERIES = get_regression_table(lm_groceries)
regression_table_GROCERIES
```
Compared with ordinary grocery stores (like Albertsons, HEB, or Krogers), convenience stores charge somewhere between 0.41 and 0.92 dollars more for the same product.”

## Part D

```{r, include=FALSE, echo=FALSE, message=FALSE}
groceries$Store <- relevel(as.factor(groceries$Store), ref = "H-E-B")
lm_store <- lm(Price ~ Product + Store, data = groceries)
coefs <- coef(lm_store)
store_coefs <- coefs[grepl("^Store", names(coefs))]
store_coefs <- c("H-E-B" = 0, store_coefs)
sorted_store_coefs <- sort(store_coefs)
store_coefs = round(sorted_store_coefs, 2)
regression_table_STORE = get_regression_table(lm_store)
store_coefs 
regression_table_STORE
```
Based on our model, Walmart and Kroger Fresh Fare offer the lowest average prices for the same products, as their store variables have the most negative coefficients, reducing the baseline price the most. In contrast, Whole Foods and Wheatsville Food Co-Op charge the highest prices, as their coefficients are the largest and most positive, increasing the intercept (Price) the most. This suggests that Walmart and Kroger Fresh Fare provide the most affordable options, while Whole Foods and Wheatsville Co-Op are comparatively more expensive for identical products.

## Part E
Although Central Market charges slightly more—approximately $0.07—for the same product compared to other stores, this difference is minimal when contrasted with stores like Whole Foods, which charge over a dollar more. This suggests that Central Market likely carries more premium products, contributing to the overall higher cost to consumers. This explanation is further supported by the data, which show that the average store in the dataset is $`r round(mean(store_coefs), 2)` more expensive than H-E-B. Moreover, because the 95% confidence interval for Central Market’s price difference includes zero, the $0.07 increase is not statistically significant.

## Part F
```{r, include=FALSE, echo=FALSE, message=FALSE}
library(effectsize)
groceries = groceries %>%
  mutate(Income10k = Income/10000)
lm_income = lm(Price ~ Income10k + Product, data = groceries)
coef(lm_income)
confint(lm_income)
get_regression_table(lm_income)
sd_income <- sd(groceries$Income10k, na.rm = TRUE)
sd_price <- sd(groceries$Price, na.rm = TRUE)
standarized = standardize_parameters(lm_income)
standarized

```
The coefficient for Income10k is negative, indicating that, on average, consumers in poorer ZIP codes pay slightly more for the same product, meaning that for every ten thousand dollars a person makes, they can expect to pay 0.03 cents less.

A one-standard deviation increase in the income of a ZIP code seems to be associated with
a -0.03 standard-deviation change in the price that consumers in that ZIP code expect to pay for
the same product.
\newpage

# Problem 3

## Part A

This is true, as the 95% confidence interval for the main effect of minority % on FAIR policies does not include zero, and the ceofficient is a positive number. Additionally, the P value is less than 0.05, giving strong evidence to reject the null hypothesis (that there is zero difference between minorities for FAIR policies).

## Part B

 There is not enough evidence to determine whether an interaction effect exists between minority percentage and age of housing stock. The model only includes the main effect of minority percentage (with a 95% CI from 0.009 to 0.018) and does not include age of housing or an interaction term. To evaluate this interaction, we would need a model including both main effects and their interaction, and examine whether the confidence interval for the interaction coefficient includes zero. Alternatively, we could visualize a stratified scatter plot of minority percentage versus FAIR policies with slope lines for different age groups of housing. However, without either the figure or the model, we cannot conclude whether an interaction is present.

## Part C

False, although the intercept and slope for the High Fire Risk group appears higher than that of the Low Fire Risk group using the grapy, however, the 95% CI for the interaction effect between Minority % and Fire-Risk includes zero, meaning that the difference in slopes is not actually statistically significant. 

## Part D

False, the minority coefficient decreased when income was added to the model, but it remains statistically significant (p < 0.05, CI does not include 0), thus, income does decrease the effect, but it does not eliminate it entirely. A more accurate way to assess whether income “explains away” the association between minority percentage and FAIR policy uptake would be to compare effect sizes across models. Specifically, we could create a table showing the change in the coefficient for minority percentage and the change in model R² when income is added as a control variable. This would allow us to evaluate how much of the variation in the number of FAIR policies is accounted for by income, and how much of the original effect attributed to minority percentage remains.

## Part E

True. In the full model model_E, which includes minority, income, fire, and age, the coefficient for minority remains statistically significant (estimate = 0.008, p = 0.006), with a 95% CI that does not include zero.
This indicates an association between minority percentage and FAIR policy uptake (about a 0.008 increase on avergae per each percent increased in minority percentage), even after holding the other variables constant.
